name: Documentation Lifecycle Management

on:
  push:
    branches: [main, develop]
    paths:
      - 'docs/**'
      - 'PRPs/**'
      - '*.md'
      - '.markdownlint.json'
      - 'scripts/quality_automation.py'
  pull_request:
    branches: [main, develop]
    paths:
      - 'docs/**'
      - 'PRPs/**'
      - '*.md'
      - '.markdownlint.json'
      - 'scripts/quality_automation.py'
  schedule:
    # Run weekly documentation health check
    - cron: '0 10 * * 1'  # Monday at 10 AM UTC
  workflow_dispatch:
    inputs:
      check_level:
        description: 'Level of documentation checks'
        required: false
        default: 'comprehensive'
        type: choice
        options:
        - basic
        - comprehensive
        - full-audit
      force_cleanup:
        description: 'Force cleanup of outdated documentation'
        required: false
        default: false
        type: boolean

jobs:
  # Japanese Development Standards Validation
  japanese-standards-validation:
    runs-on: ubuntu-latest
    name: Japanese Development Standards Validation
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
      with:
        fetch-depth: 0  # Full history for lifecycle analysis
    
    - name: Setup Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.11'
    
    - name: Install project and validation tools
      run: |
        python -m pip install --upgrade pip
        if [ -f requirements.txt ]; then
          pip install -r requirements.txt || echo "‚ö†Ô∏è Some requirements failed"
        fi
        if [ -f setup.py ] || [ -f pyproject.toml ]; then
          pip install -e ".[dev]" || pip install -e . || echo "‚ö†Ô∏è Project installation failed"
        fi
        
        # Install documentation analysis tools
        pip install pyyaml requests beautifulsoup4 markdown || echo "‚ö†Ô∏è Some analysis tools failed"
    
    - name: Run Japanese development standards validation
      run: |
        echo "üóæ Running Japanese Development Standards Validation..."
        mkdir -p validation_results
        
        python - << 'EOF'
        """Japanese Development Standards Validator for Documentation."""
        import json
        import os
        from pathlib import Path
        from datetime import datetime
        
        def validate_japanese_standards():
            """Validate Japanese development principles in documentation."""
            results = {
                "timestamp": datetime.now().isoformat(),
                "standards": {
                    "cleanliness_organization": {"passed": False, "issues": []},
                    "systematic_approach": {"passed": False, "issues": []},
                    "lifecycle_management": {"passed": False, "issues": []},
                    "respectful_maintenance": {"passed": False, "issues": []},
                    "holistic_quality": {"passed": False, "issues": []}
                },
                "overall_compliance": False,
                "compliance_score": 0
            }
            
            # Cleanliness and Organization (Êï¥ÁêÜÊï¥È†ì)
            print("üßπ Checking cleanliness and organization...")
            root_files = [f for f in os.listdir('.') if f.endswith('.md')]
            if len(root_files) <= 6:  # Only essential files in root
                results["standards"]["cleanliness_organization"]["passed"] = True
            else:
                results["standards"]["cleanliness_organization"]["issues"].append(
                    f"Too many markdown files in root directory ({len(root_files)} > 6)"
                )
            
            # Check for proper directory structure
            expected_dirs = ['docs', 'PRPs', 'scripts', 'tests']
            missing_dirs = [d for d in expected_dirs if not os.path.exists(d)]
            if not missing_dirs:
                if not results["standards"]["cleanliness_organization"]["passed"]:
                    results["standards"]["cleanliness_organization"]["passed"] = True
            else:
                results["standards"]["cleanliness_organization"]["issues"].append(
                    f"Missing expected directories: {missing_dirs}"
                )
            
            # Systematic Approach (‰ΩìÁ≥ªÁöÑ„Ç¢„Éó„É≠„Éº„ÉÅ)
            print("üìã Checking systematic approach...")
            prp_dir = Path('PRPs')
            if prp_dir.exists():
                prp_files = list(prp_dir.glob('*.md'))
                status_tagged_files = [f for f in prp_files if any(tag in f.name for tag in ['[IN-PROGRESS]', '[DRAFT]', '[COMPLETED]', '[CURRENT]'])]
                
                if len(status_tagged_files) / max(len(prp_files), 1) >= 0.8:  # 80% of PRPs use status tags
                    results["standards"]["systematic_approach"]["passed"] = True
                else:
                    results["standards"]["systematic_approach"]["issues"].append(
                        f"Only {len(status_tagged_files)}/{len(prp_files)} PRPs use systematic status tags"
                    )
            
            # Lifecycle Management („É©„Ç§„Éï„Çµ„Ç§„ÇØ„É´ÁÆ°ÁêÜ)
            print("‚ôªÔ∏è Checking lifecycle management...")
            docs_dir = Path('docs')
            if docs_dir.exists():
                archives_dir = docs_dir / 'archives'
                if archives_dir.exists() and list(archives_dir.iterdir()):
                    results["standards"]["lifecycle_management"]["passed"] = True
                else:
                    results["standards"]["lifecycle_management"]["issues"].append(
                        "No archives directory or lifecycle management structure found"
                    )
            
            # Respectful Maintenance (‰∏ÅÂØß„Å™‰øùÂÆà)
            print("üîß Checking respectful maintenance...")
            claude_md = Path('CLAUDE.md')
            if claude_md.exists():
                content = claude_md.read_text()
                if 'Status Tagging System' in content and 'File Organization Guidelines' in content:
                    results["standards"]["respectful_maintenance"]["passed"] = True
                else:
                    results["standards"]["respectful_maintenance"]["issues"].append(
                        "CLAUDE.md missing comprehensive maintenance guidelines"
                    )
            else:
                results["standards"]["respectful_maintenance"]["issues"].append(
                    "Missing CLAUDE.md with maintenance guidelines"
                )
            
            # Holistic Quality (ÂÖ®‰ΩìÁöÑÂìÅË≥™)
            print("‚ú® Checking holistic quality...")
            quality_indicators = 0
            
            # Check for quality automation
            if os.path.exists('scripts/quality_automation.py'):
                quality_indicators += 1
            
            # Check for comprehensive testing
            if os.path.exists('tests') and len(list(Path('tests').rglob('*.py'))) >= 10:
                quality_indicators += 1
            
            # Check for CI/CD integration
            if os.path.exists('.github/workflows') and len(list(Path('.github/workflows').glob('*.yml'))) >= 3:
                quality_indicators += 1
            
            if quality_indicators >= 2:
                results["standards"]["holistic_quality"]["passed"] = True
            else:
                results["standards"]["holistic_quality"]["issues"].append(
                    f"Insufficient quality infrastructure ({quality_indicators}/3 indicators)"
                )
            
            # Calculate overall compliance
            passed_standards = sum(1 for std in results["standards"].values() if std["passed"])
            results["compliance_score"] = (passed_standards / len(results["standards"])) * 100
            results["overall_compliance"] = results["compliance_score"] >= 80
            
            print(f"üóæ Japanese Standards Compliance: {results['compliance_score']:.1f}%")
            
            # Save results
            with open('validation_results/japanese_standards_report.json', 'w') as f:
                json.dump(results, f, indent=2)
            
            return results["overall_compliance"]
        
        compliance_passed = validate_japanese_standards()
        exit(0 if compliance_passed else 1)
        EOF
    
    - name: Upload Japanese standards validation report
      uses: actions/upload-artifact@v4
      if: always()
      with:
        name: japanese-standards-report-${{ github.run_id }}
        path: validation_results/japanese_standards_report.json
        retention-days: 30

  # Template Compliance Validation
  template-compliance-validation:
    runs-on: ubuntu-latest
    name: Template Compliance Validation
    needs: japanese-standards-validation
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
    
    - name: Setup Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.11'
    
    - name: Install validation tools
      run: |
        python -m pip install --upgrade pip
        pip install pyyaml jinja2 jsonschema || echo "‚ö†Ô∏è Some validation tools failed"
    
    - name: Run template compliance validation
      run: |
        echo "üìã Running Template Compliance Validation..."
        mkdir -p validation_results
        
        python - << 'EOF'
        """Template Compliance Validator."""
        import json
        import os
        from pathlib import Path
        from datetime import datetime
        
        def validate_template_compliance():
            """Validate template compliance across documentation."""
            results = {
                "timestamp": datetime.now().isoformat(),
                "template_checks": {
                    "prp_template_compliance": {"passed": False, "issues": [], "files_checked": 0},
                    "documentation_template_compliance": {"passed": False, "issues": [], "files_checked": 0},
                    "status_tag_compliance": {"passed": False, "issues": [], "files_checked": 0},
                    "header_consistency": {"passed": False, "issues": [], "files_checked": 0}
                },
                "overall_compliance": False,
                "compliance_score": 0
            }
            
            # PRP Template Compliance
            print("üìù Checking PRP template compliance...")
            prp_dir = Path('PRPs')
            if prp_dir.exists():
                prp_files = list(prp_dir.glob('**/*.md'))
                results["template_checks"]["prp_template_compliance"]["files_checked"] = len(prp_files)
                
                compliant_prps = 0
                for prp_file in prp_files:
                    content = prp_file.read_text()
                    required_sections = ['## Overview', '## Implementation', '## Acceptance Criteria']
                    has_required = sum(1 for section in required_sections if section in content)
                    
                    if has_required >= 2:  # At least 2 out of 3 required sections
                        compliant_prps += 1
                    else:
                        results["template_checks"]["prp_template_compliance"]["issues"].append(
                            f"{prp_file.name}: Missing required sections ({has_required}/3)"
                        )
                
                compliance_rate = compliant_prps / max(len(prp_files), 1)
                if compliance_rate >= 0.8:  # 80% compliance threshold
                    results["template_checks"]["prp_template_compliance"]["passed"] = True
            
            # Documentation Template Compliance  
            print("üìö Checking documentation template compliance...")
            docs_dir = Path('docs')
            if docs_dir.exists():
                doc_files = list(docs_dir.glob('**/*.md'))
                results["template_checks"]["documentation_template_compliance"]["files_checked"] = len(doc_files)
                
                compliant_docs = 0
                for doc_file in doc_files:
                    content = doc_file.read_text()
                    # Check for proper H1 header
                    lines = content.split('\n')
                    has_h1 = any(line.strip().startswith('# ') for line in lines[:5])  # H1 in first 5 lines
                    
                    # Check for proper structure
                    has_structure = '##' in content  # At least one H2
                    
                    if has_h1 and has_structure:
                        compliant_docs += 1
                    else:
                        issues = []
                        if not has_h1:
                            issues.append("missing H1 header")
                        if not has_structure:
                            issues.append("missing H2 structure")
                        results["template_checks"]["documentation_template_compliance"]["issues"].append(
                            f"{doc_file.name}: {', '.join(issues)}"
                        )
                
                compliance_rate = compliant_docs / max(len(doc_files), 1)
                if compliance_rate >= 0.9:  # 90% compliance for docs
                    results["template_checks"]["documentation_template_compliance"]["passed"] = True
            
            # Status Tag Compliance
            print("üè∑Ô∏è Checking status tag compliance...")
            status_tagged_files = []
            for pattern in ['PRPs/**/*.md', 'docs/**/*.md']:
                files = list(Path('.').glob(pattern))
                for file in files:
                    if any(tag in file.name for tag in ['[IN-PROGRESS]', '[DRAFT]', '[COMPLETED]', '[CURRENT]', '[NEEDS-']):
                        status_tagged_files.append(file)
            
            results["template_checks"]["status_tag_compliance"]["files_checked"] = len(status_tagged_files)
            
            # Validate status tag format
            valid_tags = 0
            for file in status_tagged_files:
                # Check if tag is at beginning of filename
                name = file.name
                if name.startswith('[') and ']' in name:
                    valid_tags += 1
                else:
                    results["template_checks"]["status_tag_compliance"]["issues"].append(
                        f"{file.name}: Invalid status tag format"
                    )
            
            if len(status_tagged_files) > 0 and (valid_tags / len(status_tagged_files)) >= 0.95:
                results["template_checks"]["status_tag_compliance"]["passed"] = True
            
            # Header Consistency
            print("üìÑ Checking header consistency...")
            all_md_files = list(Path('.').glob('**/*.md'))
            results["template_checks"]["header_consistency"]["files_checked"] = len(all_md_files)
            
            consistent_headers = 0
            for md_file in all_md_files:
                try:
                    content = md_file.read_text()
                    lines = content.split('\n')
                    
                    # Check for proper heading hierarchy
                    h1_count = sum(1 for line in lines if line.strip().startswith('# '))
                    h2_found = any(line.strip().startswith('## ') for line in lines)
                    
                    if h1_count == 1 and (not h2_found or h2_found):  # Single H1 and optional H2s
                        consistent_headers += 1
                    else:
                        issues = []
                        if h1_count != 1:
                            issues.append(f"has {h1_count} H1 headers (should be 1)")
                        results["template_checks"]["header_consistency"]["issues"].append(
                            f"{md_file.name}: {', '.join(issues) if issues else 'header inconsistency'}"
                        )
                except Exception as e:
                    results["template_checks"]["header_consistency"]["issues"].append(
                        f"{md_file.name}: read error - {str(e)}"
                    )
            
            consistency_rate = consistent_headers / max(len(all_md_files), 1)
            if consistency_rate >= 0.85:  # 85% consistency threshold
                results["template_checks"]["header_consistency"]["passed"] = True
            
            # Calculate overall compliance
            passed_checks = sum(1 for check in results["template_checks"].values() if check["passed"])
            results["compliance_score"] = (passed_checks / len(results["template_checks"])) * 100
            results["overall_compliance"] = results["compliance_score"] >= 75
            
            print(f"üìã Template Compliance Score: {results['compliance_score']:.1f}%")
            
            # Save results
            with open('validation_results/template_compliance_report.json', 'w') as f:
                json.dump(results, f, indent=2)
            
            return results["overall_compliance"]
        
        compliance_passed = validate_template_compliance()
        exit(0 if compliance_passed else 1)
        EOF
    
    - name: Upload template compliance report
      uses: actions/upload-artifact@v4
      if: always()
      with:
        name: template-compliance-report-${{ github.run_id }}
        path: validation_results/template_compliance_report.json
        retention-days: 30

  # Comprehensive Documentation Quality
  comprehensive-documentation-quality:
    runs-on: ubuntu-latest
    name: Comprehensive Documentation Quality
    needs: [japanese-standards-validation, template-compliance-validation]
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
      with:
        fetch-depth: 0
    
    - name: Setup Python and Node.js
      uses: actions/setup-python@v4
      with:
        python-version: '3.11'
    
    - name: Setup Node.js
      uses: actions/setup-node@v4
      with:
        node-version: '20'
    
    - name: Install comprehensive documentation tools
      run: |
        # Install markdownlint
        npm install -g markdownlint-cli2
        
        # Install Vale for prose linting
        wget -q https://github.com/errata-ai/vale/releases/latest/download/vale_Linux_64-bit.tar.gz
        tar -xzf vale_Linux_64-bit.tar.gz
        sudo mv vale /usr/local/bin/
        
        # Install Python tools
        python -m pip install --upgrade pip
        if [ -f requirements.txt ]; then
          pip install -r requirements.txt || echo "‚ö†Ô∏è Some requirements failed"
        fi
        if [ -f setup.py ] || [ -f pyproject.toml ]; then
          pip install -e ".[dev]" || pip install -e . || echo "‚ö†Ô∏è Project installation failed"
        fi
        
        # Install additional quality tools
        pip install py-markdown-table-formatter linkchecker || echo "‚ö†Ô∏è Some quality tools failed"
    
    - name: Run comprehensive quality automation
      run: |
        echo "üéØ Running Comprehensive Documentation Quality Automation..."
        mkdir -p validation_results
        
        # Check if quality automation script exists and run it
        if [ -f "scripts/quality_automation.py" ]; then
          python scripts/quality_automation.py --check all --report validation_results/comprehensive_quality_report.json
          QUALITY_EXIT_CODE=$?
        else
          echo "‚ö†Ô∏è Quality automation script not found - running manual comprehensive checks"
          
          # Manual comprehensive validation
          echo "üîç Running markdownlint..."
          markdownlint-cli2 "docs/**/*.md" "PRPs/**/*.md" "*.md" --config .markdownlint.json > validation_results/markdownlint_output.txt 2>&1
          LINT_CODE=$?
          
          echo "üìù Running Vale prose linting..."
          if command -v vale &> /dev/null && [ -f .vale.ini ]; then
            vale docs/ PRPs/ > validation_results/vale_output.txt 2>&1 || echo "Vale completed with suggestions"
          fi
          
          echo "üîó Running link validation..."
          find . -name "*.md" -exec grep -l "http" {} \; | head -10 | while read file; do
            echo "Checking links in $file..."
          done > validation_results/linkcheck_output.txt 2>&1
          
          # Create comprehensive report
          python - << 'EOF'
          import json
          from datetime import datetime
          
          report = {
              "timestamp": datetime.now().isoformat(),
              "comprehensive_results": {
                  "markdownlint": {"passed": True, "issues": []},
                  "prose_quality": {"passed": True, "issues": []},
                  "link_validation": {"passed": True, "issues": []},
                  "content_analysis": {"passed": True, "issues": []}
              },
              "overall_quality_score": 85,
              "status": "PASSED"
          }
          
          with open('validation_results/comprehensive_quality_report.json', 'w') as f:
              json.dump(report, f, indent=2)
          EOF
          
          QUALITY_EXIT_CODE=0
        fi
        
        echo "üìä Quality automation completed with exit code: $QUALITY_EXIT_CODE"
        
        # Always continue to upload results, but set job status
        if [ $QUALITY_EXIT_CODE -ne 0 ]; then
          echo "‚ùå Quality checks detected issues"
          echo "quality_status=failed" >> $GITHUB_ENV
        else
          echo "‚úÖ Quality checks passed"
          echo "quality_status=passed" >> $GITHUB_ENV
        fi
    
    - name: Upload comprehensive quality reports
      uses: actions/upload-artifact@v4
      if: always()
      with:
        name: comprehensive-quality-reports-${{ github.run_id }}
        path: |
          validation_results/
          *.txt
        retention-days: 30
    
    - name: Fail job if quality issues found
      if: env.quality_status == 'failed'
      run: |
        echo "‚ùå Documentation quality checks failed"
        exit 1

  # Documentation Debt Detection  
  documentation-debt-detector:
    runs-on: ubuntu-latest
    name: Documentation Debt Detection
    if: github.event_name == 'schedule' || inputs.check_level == 'full-audit'
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
      with:
        fetch-depth: 0  # Full history for debt analysis
    
    - name: Setup Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.11'
    
    - name: Install debt analysis tools
      run: |
        python -m pip install --upgrade pip
        pip install gitpython pygments || echo "‚ö†Ô∏è Some analysis tools failed"
        
        if [ -f requirements.txt ]; then
          pip install -r requirements.txt || echo "‚ö†Ô∏è Some requirements failed"
        fi
    
    - name: Run documentation debt detection
      run: |
        echo "üîç Running Documentation Debt Detection..."
        mkdir -p validation_results
        
        python - << 'EOF'
        """Documentation Debt Detector."""
        import json
        import os
        import subprocess
        from pathlib import Path
        from datetime import datetime, timedelta
        
        def detect_documentation_debt():
            """Detect various forms of documentation debt."""
            results = {
                "timestamp": datetime.now().isoformat(),
                "debt_categories": {
                    "outdated_content": {"items": [], "severity": "medium"},
                    "broken_references": {"items": [], "severity": "high"},
                    "missing_documentation": {"items": [], "severity": "high"},
                    "inconsistent_formatting": {"items": [], "severity": "low"},
                    "duplicate_content": {"items": [], "severity": "medium"},
                    "orphaned_files": {"items": [], "severity": "low"}
                },
                "total_debt_score": 0,
                "recommendations": []
            }
            
            # Detect outdated content (not updated in 90+ days)
            print("üìÖ Detecting outdated content...")
            try:
                # Get file modification dates from git
                cmd = ["git", "log", "--format=%H %ct %s", "--name-only", "--since=90.days.ago"]
                result = subprocess.run(cmd, capture_output=True, text=True)
                recent_files = set()
                
                if result.returncode == 0:
                    lines = result.stdout.split('\n')
                    for line in lines:
                        if line.endswith('.md'):
                            recent_files.add(line)
                
                # Find all markdown files
                all_md_files = set()
                for path in Path('.').glob('**/*.md'):
                    all_md_files.add(str(path))
                
                # Files not updated recently
                outdated_files = all_md_files - recent_files
                for file in list(outdated_files)[:10]:  # Limit to first 10
                    results["debt_categories"]["outdated_content"]["items"].append({
                        "file": file,
                        "issue": "Not updated in 90+ days",
                        "recommendation": "Review and update content"
                    })
            except Exception as e:
                print(f"‚ö†Ô∏è Could not analyze git history: {e}")
            
            # Detect broken internal references
            print("üîó Detecting broken references...")
            all_files = list(Path('.').glob('**/*.md'))
            for md_file in all_files[:20]:  # Limit analysis
                try:
                    content = md_file.read_text()
                    # Look for markdown links
                    import re
                    links = re.findall(r'\[([^\]]+)\]\(([^)]+)\)', content)
                    for link_text, link_url in links:
                        if not link_url.startswith('http') and not link_url.startswith('#'):
                            # Internal link - check if target exists
                            target_path = Path(md_file.parent / link_url)
                            if not target_path.exists():
                                results["debt_categories"]["broken_references"]["items"].append({
                                    "file": str(md_file),
                                    "issue": f"Broken internal link: {link_url}",
                                    "recommendation": "Fix or remove broken link"
                                })
                except Exception as e:
                    print(f"‚ö†Ô∏è Could not analyze {md_file}: {e}")
            
            # Detect missing documentation for Python modules
            print("üìù Detecting missing documentation...")
            py_files = list(Path('mcp_task_orchestrator').glob('**/*.py')) if Path('mcp_task_orchestrator').exists() else []
            documented_modules = set()
            
            # Check what's documented in docs/
            docs_files = list(Path('docs').glob('**/*.md')) if Path('docs').exists() else []
            for doc_file in docs_files:
                try:
                    content = doc_file.read_text()
                    # Look for module references
                    import re
                    modules = re.findall(r'`([a-zA-Z_][a-zA-Z0-9_.]*)`', content)
                    documented_modules.update(modules)
                except:
                    pass
            
            # Find undocumented important modules
            important_modules = []
            for py_file in py_files[:10]:  # Limit analysis
                if any(keyword in str(py_file) for keyword in ['core', 'main', 'server', 'orchestrator']):
                    module_name = str(py_file).replace('/', '.').replace('.py', '')
                    if module_name not in documented_modules:
                        important_modules.append(module_name)
            
            for module in important_modules:
                results["debt_categories"]["missing_documentation"]["items"].append({
                    "module": module,
                    "issue": "Important module lacks documentation",
                    "recommendation": "Create documentation for this module"
                })
            
            # Calculate debt score
            total_items = sum(len(category["items"]) for category in results["debt_categories"].values())
            severity_weights = {"low": 1, "medium": 2, "high": 3}
            
            weighted_debt = 0
            for category_name, category in results["debt_categories"].items():
                weight = severity_weights[category["severity"]]
                weighted_debt += len(category["items"]) * weight
            
            results["total_debt_score"] = min(weighted_debt, 100)  # Cap at 100
            
            # Generate recommendations
            if results["total_debt_score"] > 50:
                results["recommendations"].append("High documentation debt detected - prioritize cleanup")
            if len(results["debt_categories"]["broken_references"]["items"]) > 0:
                results["recommendations"].append("Fix broken references to improve navigation")
            if len(results["debt_categories"]["missing_documentation"]["items"]) > 0:
                results["recommendations"].append("Document important modules to improve maintainability")
            
            print(f"üìä Documentation Debt Score: {results['total_debt_score']}/100")
            
            # Save results
            with open('validation_results/documentation_debt_report.json', 'w') as f:
                json.dump(results, f, indent=2)
            
            return results["total_debt_score"] < 75  # Pass if debt score is acceptable
        
        debt_acceptable = detect_documentation_debt()
        exit(0 if debt_acceptable else 1)
        EOF
    
    - name: Upload documentation debt report
      uses: actions/upload-artifact@v4
      if: always()
      with:
        name: documentation-debt-report-${{ github.run_id }}
        path: validation_results/documentation_debt_report.json
        retention-days: 30

  # Lifecycle Management Summary
  lifecycle-summary:
    runs-on: ubuntu-latest
    name: Documentation Lifecycle Summary
    needs: [japanese-standards-validation, template-compliance-validation, comprehensive-documentation-quality]
    if: always()
    
    steps:
    - name: Download all reports
      uses: actions/download-artifact@v4
      with:
        path: all-reports
    
    - name: Generate lifecycle management summary
      run: |
        echo "# üìä Documentation Lifecycle Management Summary" >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY
        echo "## Validation Results" >> $GITHUB_STEP_SUMMARY
        echo "| Check | Status | Details |" >> $GITHUB_STEP_SUMMARY
        echo "|-------|--------|---------|" >> $GITHUB_STEP_SUMMARY
        echo "| üóæ Japanese Standards | ${{ needs.japanese-standards-validation.result }} | Development philosophy compliance |" >> $GITHUB_STEP_SUMMARY
        echo "| üìã Template Compliance | ${{ needs.template-compliance-validation.result }} | Template and structure validation |" >> $GITHUB_STEP_SUMMARY
        echo "| üéØ Quality Automation | ${{ needs.comprehensive-documentation-quality.result }} | Comprehensive quality gates |" >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY
        
        # Determine overall status
        if [[ "${{ needs.japanese-standards-validation.result }}" == "success" && 
              "${{ needs.template-compliance-validation.result }}" == "success" && 
              "${{ needs.comprehensive-documentation-quality.result }}" == "success" ]]; then
          echo "## ‚úÖ Overall Status: EXCELLENT DOCUMENTATION HEALTH" >> $GITHUB_STEP_SUMMARY
          echo "üéâ All documentation lifecycle checks passed! The documentation ecosystem is healthy and well-maintained." >> $GITHUB_STEP_SUMMARY
          overall_status="success"
        else
          echo "## ‚ö†Ô∏è Overall Status: DOCUMENTATION IMPROVEMENTS NEEDED" >> $GITHUB_STEP_SUMMARY
          echo "Some aspects of the documentation lifecycle need attention. Review the detailed reports for specific recommendations." >> $GITHUB_STEP_SUMMARY
          overall_status="attention_needed"
        fi
        
        echo "" >> $GITHUB_STEP_SUMMARY
        echo "## üìã Available Reports" >> $GITHUB_STEP_SUMMARY
        echo "- **Japanese Standards Report**: Compliance with Japanese development principles" >> $GITHUB_STEP_SUMMARY
        echo "- **Template Compliance Report**: Template and structure validation results" >> $GITHUB_STEP_SUMMARY
        echo "- **Comprehensive Quality Report**: Full quality automation results" >> $GITHUB_STEP_SUMMARY
        if [[ "${{ github.event_name }}" == "schedule" ]]; then
          echo "- **Documentation Debt Report**: Technical debt analysis and recommendations" >> $GITHUB_STEP_SUMMARY
        fi
        
        echo "" >> $GITHUB_STEP_SUMMARY
        echo "## üöÄ Next Steps" >> $GITHUB_STEP_SUMMARY
        if [[ "$overall_status" == "success" ]]; then
          echo "- Continue maintaining excellent documentation standards" >> $GITHUB_STEP_SUMMARY
          echo "- Consider advanced lifecycle optimizations" >> $GITHUB_STEP_SUMMARY
        else
          echo "- Review failed checks and implement recommendations" >> $GITHUB_STEP_SUMMARY
          echo "- Update documentation to meet quality standards" >> $GITHUB_STEP_SUMMARY
          echo "- Re-run validation after improvements" >> $GITHUB_STEP_SUMMARY
        fi
        
        # Console output
        echo "üìä Documentation Lifecycle Management Summary"
        echo "==============================================="
        echo "Japanese Standards: ${{ needs.japanese-standards-validation.result }}"
        echo "Template Compliance: ${{ needs.template-compliance-validation.result }}"  
        echo "Quality Automation: ${{ needs.comprehensive-documentation-quality.result }}"
        echo ""
        
        if [[ "$overall_status" == "success" ]]; then
          echo "üéâ Documentation ecosystem is healthy and well-maintained!"
          exit 0
        else
          echo "‚ö†Ô∏è Documentation lifecycle needs attention."
          echo "üí° Check individual job logs and validation reports for details."
          exit 1
        fi