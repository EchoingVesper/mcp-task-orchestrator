"""
Persistence management for the MCP Task Orchestrator.

This module handles the persistence of task state and configuration data
to prevent task loss during restarts or context resets.
"""

import os
import json
import shutil
import logging
import time
from pathlib import Path
from typing import Dict, List, Optional, Any, Union
import filelock
from datetime import datetime

from .orchestrator.models import TaskBreakdown, SubTask, TaskStatus, SpecialistType, ComplexityLevel

logger = logging.getLogger("mcp_task_orchestrator.persistence")

class PersistenceManager:
    """Manages persistence of task state and configuration data."""
    
    # Directory structure constants
    PERSISTENCE_DIR = ".task_orchestrator"
    ROLES_DIR = "roles"
    TASKS_DIR = "tasks"
    ACTIVE_TASKS_DIR = "active"
    ARCHIVE_TASKS_DIR = "archive"
    LOCKS_DIR = "locks"
    LOGS_DIR = "logs"
    
    # File name constants
    DEFAULT_ROLES_FILE = "default_roles.yaml"
    CUSTOM_ROLES_FILE = "custom_roles.yaml"
    
    def __init__(self, base_dir: Optional[str] = None):
        """Initialize the persistence manager.
        
        Args:
            base_dir: Base directory for the persistence storage.
                     If None, uses the current working directory.
        """
        if base_dir is None:
            # Check environment variable first
            base_dir = os.environ.get("MCP_TASK_ORCHESTRATOR_BASE_DIR")
            
            if not base_dir:
                # Default to the directory containing the package
                base_dir = Path(__file__).parent.parent
        
        self.base_dir = Path(base_dir)
        self.persistence_dir = self.base_dir / self.PERSISTENCE_DIR
        
        # Initialize directory structure
        self._initialize_directory_structure()
        
        # Configure logging
        self._setup_logging()
    
    def _initialize_directory_structure(self) -> None:
        """Create the directory structure for persistence if it doesn't exist."""
        # Create main persistence directory
        self.persistence_dir.mkdir(exist_ok=True)
        
        # Create subdirectories
        (self.persistence_dir / self.ROLES_DIR).mkdir(exist_ok=True)
        (self.persistence_dir / self.TASKS_DIR).mkdir(exist_ok=True)
        (self.persistence_dir / self.TASKS_DIR / self.ACTIVE_TASKS_DIR).mkdir(exist_ok=True)
        (self.persistence_dir / self.TASKS_DIR / self.ARCHIVE_TASKS_DIR).mkdir(exist_ok=True)
        (self.persistence_dir / self.LOCKS_DIR).mkdir(exist_ok=True)
        (self.persistence_dir / self.LOGS_DIR).mkdir(exist_ok=True)
        
        logger.info(f"Initialized persistence directory structure at {self.persistence_dir}")
    
    def _setup_logging(self) -> None:
        """Set up logging for the persistence manager."""
        log_file = self.persistence_dir / self.LOGS_DIR / "persistence.log"
        
        # Add a file handler to the logger
        file_handler = logging.FileHandler(log_file)
        file_handler.setFormatter(logging.Formatter(
            "%(asctime)s - %(name)s - %(levelname)s - %(message)s"
        ))
        
        logger.addHandler(file_handler)
        logger.info("Persistence logging initialized")
    
    def get_roles_directory(self) -> Path:
        """Get the path to the roles directory."""
        return self.persistence_dir / self.ROLES_DIR
    
    def get_active_tasks_directory(self) -> Path:
        """Get the path to the active tasks directory."""
        return self.persistence_dir / self.TASKS_DIR / self.ACTIVE_TASKS_DIR
    
    def get_archive_tasks_directory(self) -> Path:
        """Get the path to the archived tasks directory."""
        return self.persistence_dir / self.TASKS_DIR / self.ARCHIVE_TASKS_DIR
    
    def get_lock_file_path(self, resource_name: str) -> Path:
        """Get the path to a lock file for a specific resource."""
        return self.persistence_dir / self.LOCKS_DIR / f"{resource_name}.lock"
    
    def acquire_lock(self, resource_name: str, timeout: int = 30) -> filelock.FileLock:
        """Acquire a lock for a specific resource.
        
        Args:
            resource_name: Name of the resource to lock
            timeout: Maximum time to wait for the lock in seconds
            
        Returns:
            A FileLock object that should be released when done
            
        Raises:
            filelock.Timeout: If the lock cannot be acquired within the timeout
        """
        # Check for and clean up any stale locks first
        self._check_and_cleanup_stale_lock(resource_name)
        
        lock_file = self.get_lock_file_path(resource_name)
        lock = filelock.FileLock(lock_file, timeout=timeout)
        
        try:
            # Use a shorter polling interval to be more responsive
            lock.acquire(timeout=timeout, poll_intervall=0.1)
            logger.debug(f"Acquired lock for {resource_name}")
            return lock
        except filelock.Timeout:
            logger.error(f"Could not acquire lock for {resource_name} within {timeout} seconds")
            # Try to force-remove the lock file if it's stale
            self._check_and_cleanup_stale_lock(resource_name, force=True)
            raise
        except Exception as e:
            logger.error(f"Error acquiring lock for {resource_name}: {str(e)}")
            raise
            
    def _check_and_cleanup_stale_lock(self, resource_name: str, force: bool = False, max_age_seconds: int = 300) -> bool:
        """Check if a lock file is stale and clean it up if needed.
        
        Args:
            resource_name: Name of the resource to check
            force: If True, removes the lock file regardless of age
            max_age_seconds: Maximum age of lock file in seconds to consider it stale
            
        Returns:
            True if a stale lock was cleaned up, False otherwise
        """
        lock_file = self.get_lock_file_path(resource_name)
        
        if not lock_file.exists():
            return False
            
        try:
            # Check if the lock file is stale
            if force or (time.time() - lock_file.stat().st_mtime > max_age_seconds):
                # Try to acquire the lock first to ensure it's really stale
                if force:
                    try:
                        lock_file.unlink()
                        logger.info(f"Forcibly removed lock file for {resource_name}")
                        return True
                    except Exception as e:
                        logger.warning(f"Failed to forcibly remove lock file for {resource_name}: {str(e)}")
                        return False
                else:
                    try:
                        # Try to acquire with a very short timeout
                        lock = filelock.FileLock(lock_file, timeout=0.1)
                        lock.acquire(timeout=0.1)
                        # If we got here, the lock was not held by anyone
                        lock.release()
                        # Now remove it
                        lock_file.unlink()
                        logger.info(f"Removed stale lock file for {resource_name}")
                        return True
                    except filelock.Timeout:
                        # Lock is still held by someone
                        logger.debug(f"Lock for {resource_name} is still active, not removing")
                        return False
                    except Exception as e:
                        logger.warning(f"Failed to check/remove lock file for {resource_name}: {str(e)}")
                        return False
        except Exception as e:
            logger.warning(f"Failed to check/remove lock file for {resource_name}: {str(e)}")
            
        return False
    
    def save_task_breakdown(self, breakdown: TaskBreakdown) -> None:
        """Save a task breakdown to persistent storage.
        
        Args:
            breakdown: The TaskBreakdown object to save
        """
        # Ensure all subtasks have proper artifacts format
        for subtask in breakdown.subtasks:
            if not hasattr(subtask, 'artifacts') or subtask.artifacts is None:
                subtask.artifacts = []
            elif not isinstance(subtask.artifacts, list):
                subtask.artifacts = [subtask.artifacts] if subtask.artifacts else []
        
        lock = None
        try:
            # Acquire a lock for this task with a reasonable timeout
            lock = self.acquire_lock(f"task_{breakdown.parent_task_id}", timeout=5)
            
            # Serialize the task breakdown
            task_data = self._serialize_task_breakdown(breakdown)
            
            # Save to file
            task_file = self.get_active_tasks_directory() / f"{breakdown.parent_task_id}.json"
            self._atomic_write_json(task_file, task_data)
            
            logger.info(f"Saved task breakdown {breakdown.parent_task_id} to {task_file}")
        except filelock.Timeout:
            logger.error(f"Timeout acquiring lock for task {breakdown.parent_task_id} when saving")
            # Continue execution even if lock acquisition fails
        except Exception as e:
            logger.error(f"Error saving task breakdown {breakdown.parent_task_id}: {str(e)}")
        finally:
            # Ensure lock is released
            if lock and lock.is_locked:
                try:
                    lock.release()
                    logger.debug(f"Lock released for task_{breakdown.parent_task_id}")
                except Exception as e:
                    logger.error(f"Error releasing lock for task {breakdown.parent_task_id}: {str(e)}")
    
    def load_task_breakdown(self, parent_task_id: str) -> Optional[TaskBreakdown]:
        """Load a task breakdown from persistent storage.
        
        Args:
            parent_task_id: ID of the parent task to load
            
        Returns:
            The TaskBreakdown object if found, None otherwise
        """
        lock = None
        try:
            # Acquire a lock for this task with a reasonable timeout
            lock = self.acquire_lock(f"task_{parent_task_id}", timeout=15)
            
            # Check active tasks
            task_file = self.get_active_tasks_directory() / f"{parent_task_id}.json"
            
            if not task_file.exists():
                # Check archived tasks
                task_file = self.get_archive_tasks_directory() / f"{parent_task_id}.json"
                
                if not task_file.exists():
                    logger.warning(f"Task breakdown {parent_task_id} not found in persistent storage")
                    return None
            
            # Load from file
            try:
                task_data = self._read_json(task_file)
                breakdown = self._deserialize_task_breakdown(task_data)
                logger.info(f"Loaded task breakdown {parent_task_id} from {task_file}")
                return breakdown
            except Exception as e:
                logger.error(f"Error loading task breakdown {parent_task_id}: {str(e)}")
                return None
        except filelock.Timeout:
            logger.error(f"Timeout acquiring lock for task {parent_task_id} when loading")
            return None
        except Exception as e:
            logger.error(f"Error loading task breakdown {parent_task_id}: {str(e)}")
            return None
        finally:
            # Ensure lock is released
            if lock and lock.is_locked:
                try:
                    lock.release()
                    logger.debug(f"Lock released for task_{parent_task_id}")
                except Exception as e:
                    logger.error(f"Error releasing lock for task {parent_task_id}: {str(e)}")
    
    def update_subtask(self, subtask: SubTask, parent_task_id: str) -> None:
        """Update a subtask within a task breakdown.
        
        Args:
            subtask: The updated SubTask object
            parent_task_id: ID of the parent task
        """
        # Ensure artifacts is properly formatted
        if not hasattr(subtask, 'artifacts') or subtask.artifacts is None:
            subtask.artifacts = []
        elif not isinstance(subtask.artifacts, list):
            subtask.artifacts = [subtask.artifacts] if subtask.artifacts else []
        
        lock = None
        try:
            # Acquire a lock for this task with a reasonable timeout
            lock = self.acquire_lock(f"task_{parent_task_id}", timeout=15)
            
            # Load the task breakdown
            breakdown = self.load_task_breakdown(parent_task_id)
            
            if breakdown is None:
                logger.error(f"Cannot update subtask {subtask.task_id}: parent task {parent_task_id} not found")
                return
            
            # Update the subtask
            for i, st in enumerate(breakdown.subtasks):
                if st.task_id == subtask.task_id:
                    breakdown.subtasks[i] = subtask
                    break
            else:
                logger.warning(f"Subtask {subtask.task_id} not found in task breakdown {parent_task_id}")
                return
            
            # Save the updated task breakdown
            self.save_task_breakdown(breakdown)
            
            logger.info(f"Updated subtask {subtask.task_id} in task breakdown {parent_task_id}")
        except filelock.Timeout:
            logger.error(f"Timeout acquiring lock for task {parent_task_id} when updating subtask {subtask.task_id}")
            # Continue execution even if lock acquisition fails
        except Exception as e:
            logger.error(f"Error updating subtask {subtask.task_id} in task {parent_task_id}: {str(e)}")
        finally:
            # Ensure lock is released
            if lock and lock.is_locked:
                try:
                    lock.release()
                    logger.debug(f"Lock released for task_{parent_task_id}")
                except Exception as e:
                    logger.error(f"Error releasing lock for task {parent_task_id}: {str(e)}")
    
    def archive_task(self, parent_task_id: str) -> bool:
        """Archive a completed task.
        
        Args:
            parent_task_id: ID of the parent task to archive
            
        Returns:
            True if the task was archived successfully, False otherwise
        """
        lock = None
        try:
            # Acquire a lock for this task with a reasonable timeout
            lock = self.acquire_lock(f"task_{parent_task_id}", timeout=15)
            
            # Check if the task exists
            source_file = self.get_active_tasks_directory() / f"{parent_task_id}.json"
            
            if not source_file.exists():
                logger.warning(f"Cannot archive task {parent_task_id}: not found in active tasks")
                return False
            
            # Move to archive
            target_file = self.get_archive_tasks_directory() / f"{parent_task_id}.json"
            shutil.move(source_file, target_file)
            
            logger.info(f"Archived task {parent_task_id}")
            return True
        except filelock.Timeout:
            logger.error(f"Timeout acquiring lock for task {parent_task_id} when archiving")
            return False
        except Exception as e:
            logger.error(f"Error archiving task {parent_task_id}: {str(e)}")
            return False
        finally:
            # Ensure lock is released
            if lock and lock.is_locked:
                try:
                    lock.release()
                    logger.debug(f"Lock released for task_{parent_task_id}")
                except Exception as e:
                    logger.error(f"Error releasing lock for task {parent_task_id}: {str(e)}")
    
    def get_all_active_tasks(self) -> List[str]:
        """Get a list of all active task IDs.
        
        Returns:
            List of parent task IDs for all active tasks
        """
        active_dir = self.get_active_tasks_directory()
        return [f.stem for f in active_dir.glob("*.json")]
    
    def get_all_archived_tasks(self) -> List[str]:
        """Get a list of all archived task IDs.
        
        Returns:
            List of parent task IDs for all archived tasks
        """
        archive_dir = self.get_archive_tasks_directory()
        return [f.stem for f in archive_dir.glob("*.json")]
    
    def migrate_roles_from_config(self, config_dir: Path) -> None:
        """Migrate role configuration files from the config directory.
        
        Args:
            config_dir: Path to the config directory
        """
        source_file = config_dir / "specialists.yaml"
        target_file = self.get_roles_directory() / self.DEFAULT_ROLES_FILE
        
        if source_file.exists() and not target_file.exists():
            shutil.copy(source_file, target_file)
            logger.info(f"Migrated specialists configuration from {source_file} to {target_file}")
    
    def _serialize_task_breakdown(self, breakdown: TaskBreakdown) -> Dict:
        """Serialize a TaskBreakdown object to a dictionary.
        
        Args:
            breakdown: The TaskBreakdown object to serialize
            
        Returns:
            Dictionary representation of the TaskBreakdown
        """
        return {
            "version": "1.0",
            "parent_task_id": breakdown.parent_task_id,
            "description": breakdown.description,
            "complexity": breakdown.complexity.value,
            "context": breakdown.context,
            "created_at": breakdown.created_at.isoformat(),
            "subtasks": [self._serialize_subtask(st) for st in breakdown.subtasks]
        }
    
    def _serialize_subtask(self, subtask: SubTask) -> Dict:
        """Serialize a SubTask object to a dictionary.
        
        Args:
            subtask: The SubTask object to serialize
            
        Returns:
            Dictionary representation of the SubTask
        """
        return {
            "task_id": subtask.task_id,
            "title": subtask.title,
            "description": subtask.description,
            "specialist_type": subtask.specialist_type.value,
            "dependencies": subtask.dependencies,
            "estimated_effort": subtask.estimated_effort,
            "status": subtask.status.value,
            "results": subtask.results,
            "artifacts": subtask.artifacts,
            "created_at": subtask.created_at.isoformat(),
            "completed_at": subtask.completed_at.isoformat() if subtask.completed_at else None
        }
    
    def _deserialize_task_breakdown(self, data: Dict) -> TaskBreakdown:
        """Deserialize a dictionary to a TaskBreakdown object.
        
        Args:
            data: Dictionary representation of the TaskBreakdown
            
        Returns:
            The deserialized TaskBreakdown object
        """
        # Check version for compatibility
        version = data.get("version", "1.0")
        
        # Deserialize subtasks
        subtasks = [self._deserialize_subtask(st_data) for st_data in data["subtasks"]]
        
        # Create TaskBreakdown object
        return TaskBreakdown(
            parent_task_id=data["parent_task_id"],
            description=data["description"],
            complexity=ComplexityLevel(data["complexity"]),
            subtasks=subtasks,
            context=data.get("context", "")
        )
    
    def _deserialize_subtask(self, data: Dict) -> SubTask:
        """Deserialize a dictionary to a SubTask object.
        
        Args:
            data: Dictionary representation of the SubTask
            
        Returns:
            The deserialized SubTask object
        """
        # Parse dates
        created_at = datetime.fromisoformat(data["created_at"])
        completed_at = None
        if data["completed_at"]:
            completed_at = datetime.fromisoformat(data["completed_at"])
        
        # Ensure artifacts is always a list
        artifacts = data["artifacts"]
        if artifacts is None:
            artifacts = []
        elif not isinstance(artifacts, list):
            artifacts = [artifacts] if artifacts else []
        
        # Create SubTask object
        return SubTask(
            task_id=data["task_id"],
            title=data["title"],
            description=data["description"],
            specialist_type=SpecialistType(data["specialist_type"]),
            dependencies=data["dependencies"],
            estimated_effort=data["estimated_effort"],
            status=TaskStatus(data["status"]),
            results=data["results"],
            artifacts=artifacts,
            created_at=created_at,
            completed_at=completed_at
        )
    
    def _atomic_write_json(self, file_path: Path, data: Dict) -> None:
        """Write JSON data to a file atomically.
        
        Args:
            file_path: Path to the file
            data: Data to write
        """
        # Write to a temporary file first
        temp_file = file_path.with_suffix(".tmp")
        with open(temp_file, "w", encoding="utf-8") as f:
            json.dump(data, f, indent=2, ensure_ascii=False)
        
        # Rename to the target file (atomic operation)
        temp_file.replace(file_path)
    
    def _read_json(self, file_path: Path) -> Dict:
        """Read JSON data from a file.
        
        Args:
            file_path: Path to the file
            
        Returns:
            The parsed JSON data
        """
        with open(file_path, "r", encoding="utf-8") as f:
            return json.load(f)
    
    def cleanup_stale_locks(self, max_age_seconds: int = 3600) -> int:
        """Clean up stale lock files.
        
        Args:
            max_age_seconds: Maximum age of lock files in seconds
            
        Returns:
            Number of lock files removed
        """
        locks_dir = self.persistence_dir / self.LOCKS_DIR
        current_time = time.time()
        count = 0
        
        for lock_file in locks_dir.glob("*.lock"):
            # Check if the lock file is stale
            if current_time - lock_file.stat().st_mtime > max_age_seconds:
                try:
                    lock_file.unlink()
                    count += 1
                    logger.info(f"Removed stale lock file: {lock_file}")
                except Exception as e:
                    logger.warning(f"Failed to remove stale lock file {lock_file}: {str(e)}")
        
        return count
